"""
Resmi Gazete Web Scraper
TÃ¼rkiye Resmi Gazete'den gÃ¼nlÃ¼k iÃ§erikleri proxy Ã¼zerinden Ã§eker.
"""

import requests
from bs4 import BeautifulSoup
from dataclasses import dataclass
from typing import List, Optional
from datetime import datetime
import re
import time
import random
from urllib.parse import quote


@dataclass
class GazetteItem:
    """Resmi Gazete iÃ§erik Ã¶ÄŸesi"""
    title: str
    category: str
    link: str
    item_type: str = "htm"


@dataclass
class GazetteData:
    """GÃ¼nlÃ¼k Resmi Gazete verisi"""
    date: str
    issue_number: str
    items: List[GazetteItem]
    url: str


class ResmiGazeteScraper:
    """Resmi Gazete web scraper - proxy destekli"""

    BASE_URL = "https://www.resmigazete.gov.tr"
    TARGET_URL = f"{BASE_URL}/default.aspx"

    # Ãœcretsiz proxy servisleri
    PROXY_SERVICES = [
        lambda url: f"https://api.allorigins.win/raw?url={quote(url, safe='')}",
        lambda url: f"https://api.codetabs.com/v1/proxy?quest={quote(url, safe='')}",
        lambda url: f"https://corsproxy.io/?{quote(url, safe='')}",
    ]

    # Filtrelenecek baÅŸlÄ±klar (scraper seviyesinde)
    SKIP_TITLES = [
        'pdf gÃ¶rÃ¼ntÃ¼le', 'htm gÃ¶rÃ¼ntÃ¼le', 'gÃ¶rÃ¼ntÃ¼le', 'yazdÄ±r',
        'ana sayfa', 'iletiÅŸim', 'arama', 'arÅŸiv', 'Ã¶nceki sayÄ±',
        'tÃ¼m kategoriler', 'zaman aralÄ±ÄŸÄ±', 'son mÃ¼kerrer', 'fihrist'
    ]

    # Kategori baÅŸlÄ±klarÄ± (bÃ¼yÃ¼k-kÃ¼Ã§Ã¼k harf duyarsÄ±z)
    CATEGORY_HEADERS = [
        'YÃœRÃœTME VE Ä°DARE BÃ–LÃœMÃœ',
        'YASAMA BÃ–LÃœMÃœ',
        'YARGI BÃ–LÃœMÃœ',
        'Ä°LÃ‚N BÃ–LÃœMÃœ',
        'Ä°LAN BÃ–LÃœMÃœ',
        'MÄ°LLETLERARASI ANDLAÅMALAR',
        'HÃ‚KÄ°MLER VE SAVCILAR KURULU KARARI',
        'HÃ‚KÄ°MLER VE SAVCILAR KURULU',
        'HAKIMLER VE SAVCILAR KURULU',
        'CUMHURBAÅKANLIÄI KARARLARI',
        'CUMHURBAÅKANLIÄI',
        'BAKANLAR KURULU KARARLARI',
        'YÃ–NETMELÄ°KLER',
        'TEBLÄ°ÄLER',
        'GENELGELER',
        'KANUNLAR',
        'KANUN HÃœKMÃœNDE KARARNAME',
        'ATAMA KARARLARI',
    ]

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
        })

    def fetch_via_proxy(self, target_url: str) -> Optional[str]:
        """Proxy servisleri Ã¼zerinden iÃ§erik Ã§ek"""

        for i, proxy_fn in enumerate(self.PROXY_SERVICES):
            proxy_url = proxy_fn(target_url)
            print(f"      Proxy {i + 1}/{len(self.PROXY_SERVICES)} deneniyor...")

            try:
                response = self.session.get(proxy_url, timeout=45)
                response.raise_for_status()

                response.encoding = 'utf-8'
                content = response.text

                # Encoding dÃ¼zeltme
                try:
                    if 'Ãƒ' in content or 'Ã„' in content:
                        content = content.encode('latin-1').decode('utf-8')
                except:
                    pass

                if 'resmi' in content.lower() or 'gazete' in content.lower():
                    print(f"      Proxy {i + 1} baÅŸarÄ±lÄ±!")
                    return content
                else:
                    print(f"      Proxy {i + 1}: GeÃ§ersiz iÃ§erik")

            except requests.exceptions.Timeout:
                print(f"      Proxy {i + 1}: Timeout")
            except requests.exceptions.RequestException as e:
                print(f"      Proxy {i + 1}: {type(e).__name__}")

            time.sleep(random.uniform(1, 2))

        return None

    def fetch_direct(self, target_url: str) -> Optional[str]:
        """Direkt baÄŸlantÄ± dene"""
        try:
            response = self.session.get(target_url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            print("      Direkt baÄŸlantÄ± baÅŸarÄ±lÄ±!")
            return response.text
        except Exception as e:
            print(f"      Direkt baÄŸlantÄ± baÅŸarÄ±sÄ±z: {type(e).__name__}")
            return None

    def _should_skip_title(self, title: str) -> bool:
        """Bu baÅŸlÄ±k atlanmalÄ± mÄ±?"""
        title_lower = title.lower().strip()

        if len(title_lower) < 5:
            return True

        for skip in self.SKIP_TITLES:
            if skip in title_lower:
                return True

        if not any(c.isalpha() for c in title):
            return True

        return False

    def _normalize_text(self, text: str) -> str:
        """TÃ¼rkÃ§e karakterleri normalize et"""
        replacements = {
            'Ä°': 'I', 'Ä': 'G', 'Ãœ': 'U', 'Å': 'S', 'Ã–': 'O', 'Ã‡': 'C',
            'Ä±': 'I', 'ÄŸ': 'G', 'Ã¼': 'U', 'ÅŸ': 'S', 'Ã¶': 'O', 'Ã§': 'C',
            'Ã‚': 'A', 'Ã¢': 'A', 'Ã': 'I', 'Ã®': 'I', 'Ã›': 'U', 'Ã»': 'U',
        }
        result = text.upper()
        for old, new in replacements.items():
            result = result.replace(old, new)
        return result

    def _is_category_header(self, text: str) -> Optional[str]:
        """Metin bir kategori baÅŸlÄ±ÄŸÄ± mÄ±? EÅŸleÅŸen kategoriyi dÃ¶ndÃ¼r."""
        text = text.strip()
        if len(text) < 5 or len(text) > 100:
            return None

        text_normalized = self._normalize_text(text)

        for cat in self.CATEGORY_HEADERS:
            cat_normalized = self._normalize_text(cat)
            if cat_normalized in text_normalized:
                return text

        return None

    def parse_html(self, html: str) -> GazetteData:
        """HTML iÃ§eriÄŸini parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        items = []

        # Tarih ve sayÄ± bilgisini al
        text = soup.get_text()
        date_str = datetime.now().strftime('%d %B %Y')
        issue_number = "Bilinmiyor"

        date_match = re.search(
            r'(\d{1,2})\s+(Ocak|Åubat|Mart|Nisan|MayÄ±s|Haziran|Temmuz|AÄŸustos|EylÃ¼l|Ekim|KasÄ±m|AralÄ±k)\s+(\d{4})',
            text, re.IGNORECASE
        )
        if date_match:
            date_str = date_match.group(0)

        issue_match = re.search(r'SayÄ±\s*:\s*(\d+)', text)
        if issue_match:
            issue_number = issue_match.group(1)

        # HTML'deki tÃ¼m text iÃ§eriÄŸini satÄ±r satÄ±r iÅŸle
        # ve kategorileri linklerden Ã¶nce yakala
        current_category = "Genel"

        # Ã–nce tÃ¼m bold/underline elementleri bul ve kategorileri belirle
        all_elements = soup.find_all(['b', 'strong', 'u', 'a'])

        # Her element iÃ§in index tut
        element_categories = {}

        for i, elem in enumerate(all_elements):
            elem_text = elem.get_text(strip=True)

            # Link ise ve PDF/HTM ise ekle
            if elem.name == 'a':
                href = elem.get('href', '')
                title = elem_text

                if self._should_skip_title(title):
                    continue

                href_lower = href.lower()
                if '.pdf' in href_lower or '.htm' in href_lower:
                    if 'default.aspx' in href_lower:
                        continue

                    # Tam URL oluÅŸtur
                    if not href.startswith('http'):
                        if href.startswith('/'):
                            href = f"{self.BASE_URL}{href}"
                        else:
                            href = f"{self.BASE_URL}/{href}"

                    item_type = 'pdf' if '.pdf' in href_lower else 'htm'

                    # Her iÃ§erik iÃ§in baÅŸlÄ±ÄŸa gÃ¶re kategori belirle
                    category = self._get_category_from_title(title)

                    # Duplicate kontrolÃ¼
                    if not any(item.link == href for item in items):
                        items.append(GazetteItem(
                            title=title,
                            category=category,
                            link=href,
                            item_type=item_type
                        ))

        return GazetteData(
            date=date_str,
            issue_number=issue_number,
            items=items,
            url=self.BASE_URL
        )

    def _get_category_from_title(self, title: str) -> str:
        """BaÅŸlÄ±ÄŸa gÃ¶re kategori belirle"""
        title_upper = self._normalize_text(title)

        if 'HAKIM' in title_upper or 'SAVCI' in title_upper:
            return "HÃ‚KÄ°MLER VE SAVCILAR KURULU KARARI"
        elif 'YONETMELIK' in title_upper:
            return "YÃ–NETMELÄ°KLER"
        elif 'OZELLESTIRME' in title_upper:
            return "TEBLÄ°ÄLER"
        elif 'TEBLIG' in title_upper:
            return "TEBLÄ°ÄLER"
        elif 'KANUN' in title_upper:
            return "YASAMA BÃ–LÃœMÃœ"
        elif 'CUMHURBASKANI' in title_upper or 'CUMHURBASKANLIGI' in title_upper:
            return "CUMHURBAÅKANLIÄI KARARLARI"
        elif 'GENELGE' in title_upper:
            return "GENELGELER"
        elif 'ILAN' in title_upper or 'IHALE' in title_upper:
            return "Ä°LÃ‚N BÃ–LÃœMÃœ"
        elif 'YARGI' in title_upper:
            return "Ä°LÃ‚N BÃ–LÃœMÃœ"
        elif 'MERKEZ BANKASI' in title_upper or 'DOVIZ' in title_upper:
            return "TEBLÄ°ÄLER"
        else:
            return "DiÄŸer"

    def scrape(self) -> GazetteData:
        """Ana scraping fonksiyonu"""

        print("      Direkt baÄŸlantÄ± deneniyor...")
        content = self.fetch_direct(self.TARGET_URL)

        if not content:
            print("      Proxy servisleri deneniyor...")
            content = self.fetch_via_proxy(self.TARGET_URL)

        if not content:
            raise Exception("HiÃ§bir yÃ¶ntemle iÃ§erik alÄ±namadÄ±")

        return self.parse_html(content)


def main():
    """Test fonksiyonu"""
    scraper = ResmiGazeteScraper()

    try:
        data = scraper.scrape()
        print(f"\nTarih: {data.date}")
        print(f"SayÄ±: {data.issue_number}")
        print(f"Toplam iÃ§erik: {len(data.items)}")

        if data.items:
            print("\nÄ°Ã§erikler (kategorilere gÃ¶re):")
            categories = {}
            for item in data.items:
                if item.category not in categories:
                    categories[item.category] = []
                categories[item.category].append(item)

            for cat, cat_items in categories.items():
                print(f"\nğŸ“ {cat} ({len(cat_items)} Ã¶ÄŸe)")
                for item in cat_items[:5]:
                    title_short = item.title[:55] + "..." if len(item.title) > 55 else item.title
                    print(f"   - {title_short} [{item.item_type.upper()}]")
        else:
            print("\nUYARI: HiÃ§ iÃ§erik bulunamadÄ±!")

    except Exception as e:
        print(f"Hata: {e}")


if __name__ == "__main__":
    main()
