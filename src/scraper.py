"""
Resmi Gazete Web Scraper
TÃ¼rkiye Resmi Gazete'den gÃ¼nlÃ¼k iÃ§erikleri proxy Ã¼zerinden Ã§eker.
"""

import requests
from bs4 import BeautifulSoup
from dataclasses import dataclass
from typing import List, Optional
from datetime import datetime
import re
import time
import random
from urllib.parse import quote


@dataclass
class GazetteItem:
    """Resmi Gazete iÃ§erik Ã¶ÄŸesi"""
    title: str
    category: str
    link: str
    item_type: str = "htm"


@dataclass
class GazetteData:
    """GÃ¼nlÃ¼k Resmi Gazete verisi"""
    date: str
    issue_number: str
    items: List[GazetteItem]
    url: str


class ResmiGazeteScraper:
    """Resmi Gazete web scraper - proxy destekli"""

    BASE_URL = "https://www.resmigazete.gov.tr"
    TARGET_URL = f"{BASE_URL}/default.aspx"

    # Ãœcretsiz proxy servisleri
    PROXY_SERVICES = [
        lambda url: f"https://api.allorigins.win/raw?url={quote(url, safe='')}",
        lambda url: f"https://api.codetabs.com/v1/proxy?quest={quote(url, safe='')}",
        lambda url: f"https://corsproxy.io/?{quote(url, safe='')}",
    ]

    # Filtrelenecek baÅŸlÄ±klar (scraper seviyesinde)
    SKIP_TITLES = [
        'pdf gÃ¶rÃ¼ntÃ¼le', 'htm gÃ¶rÃ¼ntÃ¼le', 'gÃ¶rÃ¼ntÃ¼le', 'yazdÄ±r',
        'ana sayfa', 'iletiÅŸim', 'arama', 'arÅŸiv', 'Ã¶nceki sayÄ±',
        'tÃ¼m kategoriler', 'zaman aralÄ±ÄŸÄ±', 'son mÃ¼kerrer'
    ]

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
        })

    def fetch_via_proxy(self, target_url: str) -> Optional[str]:
        """Proxy servisleri Ã¼zerinden iÃ§erik Ã§ek"""

        for i, proxy_fn in enumerate(self.PROXY_SERVICES):
            proxy_url = proxy_fn(target_url)
            print(f"      Proxy {i + 1}/{len(self.PROXY_SERVICES)} deneniyor...")

            try:
                response = self.session.get(proxy_url, timeout=45)
                response.raise_for_status()

                # UTF-8 encoding zorla
                response.encoding = 'utf-8'
                content = response.text

                # Encoding dÃ¼zeltme - mojibake fix
                try:
                    if 'Ãƒ' in content or 'Ã„' in content:
                        content = content.encode('latin-1').decode('utf-8')
                except:
                    pass

                # Ä°Ã§eriÄŸin gerÃ§ekten Resmi Gazete olduÄŸunu kontrol et
                if 'resmi' in content.lower() or 'gazete' in content.lower():
                    print(f"      Proxy {i + 1} baÅŸarÄ±lÄ±!")
                    return content
                else:
                    print(f"      Proxy {i + 1}: GeÃ§ersiz iÃ§erik")

            except requests.exceptions.Timeout:
                print(f"      Proxy {i + 1}: Timeout")
            except requests.exceptions.RequestException as e:
                print(f"      Proxy {i + 1}: {type(e).__name__}")

            time.sleep(random.uniform(1, 2))

        return None

    def fetch_direct(self, target_url: str) -> Optional[str]:
        """Direkt baÄŸlantÄ± dene (lokal test iÃ§in)"""
        try:
            response = self.session.get(target_url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            print("      Direkt baÄŸlantÄ± baÅŸarÄ±lÄ±!")
            return response.text
        except Exception as e:
            print(f"      Direkt baÄŸlantÄ± baÅŸarÄ±sÄ±z: {type(e).__name__}")
            return None

    def _should_skip_title(self, title: str) -> bool:
        """Bu baÅŸlÄ±k atlanmalÄ± mÄ±?"""
        title_lower = title.lower().strip()

        # Ã‡ok kÄ±sa baÅŸlÄ±klar
        if len(title_lower) < 5:
            return True

        # UI elementleri
        for skip in self.SKIP_TITLES:
            if skip in title_lower:
                return True

        # Sadece harf/rakam olmayan baÅŸlÄ±klar
        if not any(c.isalpha() for c in title):
            return True

        return False

    def _detect_category(self, text: str) -> Optional[str]:
        """Metin bir kategori baÅŸlÄ±ÄŸÄ± mÄ±?"""
        text = text.strip()
        text_upper = text.upper()

        # Ana kategori baÅŸlÄ±klarÄ±
        main_categories = [
            'YÃœRÃœTME VE Ä°DARE BÃ–LÃœMÃœ',
            'YASAMA BÃ–LÃœMÃœ',
            'YARGI BÃ–LÃœMÃœ',
            'Ä°LÃ‚N BÃ–LÃœMÃœ',
            'Ä°LAN BÃ–LÃœMÃœ',
            'MÄ°LLETLERARASI ANDLAÅMALAR',
        ]

        # Alt kategori baÅŸlÄ±klarÄ±
        sub_categories = [
            'HÃ‚KÄ°MLER VE SAVCILAR KURULU KARARI',
            'HAKIMLER VE SAVCILAR KURULU',
            'CUMHURBAÅKANLIÄI KARARLARI',
            'CUMHURBAÅKANLIÄI',
            'BAKANLAR KURULU KARARLARI',
            'YÃ–NETMELÄ°KLER',
            'YÃ–NETMELIKLER',
            'TEBLÄ°ÄLER',
            'TEBLIÄLER',
            'GENELGELER',
            'KANUNLAR',
            'KANUN HÃœKMÃœNDE KARARNAMELER',
            'ATAMA KARARLARI',
            'YARGI Ä°LÃ‚NLARI',
            'YARGI Ä°LANLARI',
            'ARTIRMA, EKSÄ°LTME',
            'ARTIRMA EKSÄ°LTME',
            'Ã‡EÅÄ°TLÄ° Ä°LÃ‚NLAR',
            'Ã‡EÅÄ°TLÄ° Ä°LANLAR',
        ]

        for cat in main_categories + sub_categories:
            if cat in text_upper:
                return text

        return None

    def parse_html(self, html: str) -> GazetteData:
        """HTML iÃ§eriÄŸini parse et"""
        soup = BeautifulSoup(html, 'html.parser')
        items = []

        # Tarih ve sayÄ± bilgisini al
        text = soup.get_text()
        date_str = datetime.now().strftime('%d %B %Y')
        issue_number = "Bilinmiyor"

        # TÃ¼rkÃ§e tarih pattern
        date_match = re.search(
            r'(\d{1,2})\s+(Ocak|Åubat|Mart|Nisan|MayÄ±s|Haziran|Temmuz|AÄŸustos|EylÃ¼l|Ekim|KasÄ±m|AralÄ±k)\s+(\d{4})',
            text, re.IGNORECASE
        )
        if date_match:
            date_str = date_match.group(0)

        # SayÄ± pattern
        issue_match = re.search(r'SayÄ±\s*:\s*(\d+)', text)
        if issue_match:
            issue_number = issue_match.group(1)

        # Sayfadaki tÃ¼m metinleri ve linkleri sÄ±rayla iÅŸle
        current_category = "Genel"

        # Body iÃ§indeki tÃ¼m elementleri sÄ±rayla tara
        body = soup.find('body')
        if not body:
            body = soup

        for element in body.find_all(['a', 'b', 'strong', 'u', 'h1', 'h2', 'h3', 'h4', 'span', 'td', 'div']):
            elem_text = element.get_text(strip=True)

            if not elem_text:
                continue

            # Kategori kontrolÃ¼
            if element.name in ['b', 'strong', 'u', 'h1', 'h2', 'h3', 'h4']:
                detected_cat = self._detect_category(elem_text)
                if detected_cat:
                    current_category = detected_cat
                    continue

            # Span ve div iÃ§indeki bold metinler de kategori olabilir
            if element.name in ['span', 'td', 'div']:
                bold = element.find(['b', 'strong', 'u'])
                if bold:
                    bold_text = bold.get_text(strip=True)
                    detected_cat = self._detect_category(bold_text)
                    if detected_cat:
                        current_category = detected_cat

            # Link kontrolÃ¼
            if element.name == 'a':
                href = element.get('href', '')
                title = elem_text

                # Atlanacak baÅŸlÄ±klar
                if self._should_skip_title(title):
                    continue

                # PDF veya HTM linkleri
                href_lower = href.lower()
                if '.pdf' in href_lower or '.htm' in href_lower:
                    # Navigasyon linkleri atla
                    if 'default.aspx' in href_lower and '.htm' not in href_lower.replace('default.aspx', ''):
                        continue

                    # Tam URL oluÅŸtur
                    if not href.startswith('http'):
                        if href.startswith('/'):
                            href = f"{self.BASE_URL}{href}"
                        else:
                            href = f"{self.BASE_URL}/{href}"

                    item_type = 'pdf' if '.pdf' in href_lower else 'htm'

                    # Duplicate kontrolÃ¼
                    if not any(item.link == href for item in items):
                        items.append(GazetteItem(
                            title=title,
                            category=current_category,
                            link=href,
                            item_type=item_type
                        ))

        return GazetteData(
            date=date_str,
            issue_number=issue_number,
            items=items,
            url=self.BASE_URL
        )

    def scrape(self) -> GazetteData:
        """Ana scraping fonksiyonu"""

        # Ã–nce direkt baÄŸlantÄ± dene (lokal Ã§alÄ±ÅŸma iÃ§in)
        print("      Direkt baÄŸlantÄ± deneniyor...")
        content = self.fetch_direct(self.TARGET_URL)

        # Direkt baÅŸarÄ±sÄ±zsa proxy dene
        if not content:
            print("      Proxy servisleri deneniyor...")
            content = self.fetch_via_proxy(self.TARGET_URL)

        if not content:
            raise Exception("HiÃ§bir yÃ¶ntemle iÃ§erik alÄ±namadÄ±")

        return self.parse_html(content)


def main():
    """Test fonksiyonu"""
    scraper = ResmiGazeteScraper()

    try:
        data = scraper.scrape()
        print(f"\nTarih: {data.date}")
        print(f"SayÄ±: {data.issue_number}")
        print(f"Toplam iÃ§erik: {len(data.items)}")

        if data.items:
            print("\nÄ°Ã§erikler (kategorilere gÃ¶re):")
            categories = {}
            for item in data.items:
                if item.category not in categories:
                    categories[item.category] = []
                categories[item.category].append(item)

            for cat, cat_items in categories.items():
                print(f"\nğŸ“ {cat} ({len(cat_items)} Ã¶ÄŸe)")
                for item in cat_items[:5]:
                    title_short = item.title[:55] + "..." if len(item.title) > 55 else item.title
                    print(f"   - {title_short} [{item.item_type.upper()}]")
        else:
            print("\nUYARI: HiÃ§ iÃ§erik bulunamadÄ±!")

    except Exception as e:
        print(f"Hata: {e}")


if __name__ == "__main__":
    main()
